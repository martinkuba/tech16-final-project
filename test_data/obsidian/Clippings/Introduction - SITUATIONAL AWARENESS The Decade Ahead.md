---
author:
- '[[SITUATIONAL AWARENESS - The Decade Ahead]]'
created: 2024-11-11
description: Leopold Aschenbrenner, June 2024 You can see the future first in San
  Francisco. Over the past year, the talk of the town has shifted from $10 billion
  compute clusters to $100 billion clusters to trillion-dollar clusters. Every six
  months another zero is added to the boardroom plans. Behind the scenes, there’s
  a fierce scramble to
published: 2024-05-29
source: https://situational-awareness.ai/
tags:
- clippings
title: 'Introduction - SITUATIONAL AWARENESS: The Decade Ahead'
---

Leopold Aschenbrenner, June 2024

You can see the future first in San Francisco. 

Over the past year, the talk of the town has shifted from $10 billion compute clusters to $100 billion clusters to trillion-dollar clusters. Every six months another zero is added to the boardroom plans. Behind the scenes, there’s a fierce scramble to secure every power contract still available for the rest of the decade, every voltage transformer that can possibly be procured. American big business is gearing up to pour trillions of dollars into a long-unseen mobilization of American industrial might. By the end of the decade, American electricity production will have grown tens of percent; from the shale fields of Pennsylvania to the solar farms of Nevada, hundreds of millions of GPUs will hum.

The AGI race has begun. We are building machines that can think and reason. By 2025/26, these machines will outpace many college graduates. By the end of the decade, they will be smarter than you or I; we will have superintelligence, in the true sense of the word. Along the way, national security forces not seen in half a century will be unleashed, and before long, The Project will be on. If we’re lucky, we’ll be in an all-out race with the CCP; if we’re unlucky, an all-out war.

Everyone is now talking about AI, but few have the faintest glimmer of what is about to hit them. Nvidia analysts still think 2024 might be close to the peak. Mainstream pundits are stuck on the willful blindness of “it’s just predicting the next word”. They see only hype and business-as-usual; at most they entertain another internet-scale technological change. 

Before long, the world will wake up. But right now, there are perhaps a few hundred people, most of them in San Francisco and the AI labs, that have *situational awareness*. Through whatever peculiar forces of fate, I have found myself amongst them. A few years ago, these people were derided as crazy—but they trusted the trendlines, which allowed them to correctly predict the AI advances of the past few years. Whether these people are also right about the next few years remains to be seen. But these are very smart people—the smartest people I have ever met—and they are the ones building this technology. Perhaps they will be an odd footnote in history, or perhaps they will go down in history like Szilard and Oppenheimer and Teller. If they are seeing the future even close to correctly, we are in for a wild ride. 

Let me tell you what we see.

---

## **Table of Contents**

*Each essay is meant to stand on its own, though I’d strongly encourage reading the series as a whole. For a pdf version of the full essay series, [click here](https://situational-awareness.ai/wp-content/uploads/2024/06/situationalawareness.pdf "click here").*

**Introduction \[this page\]**  
History is live in San Francisco.

**I. [From GPT-4 to AGI: Counting the OOMs](https://situational-awareness.ai/from-gpt-4-to-agi/)  
**AGI by 2027 is strikingly plausible. GPT-2 to GPT-4 took us from ~preschooler to ~smart high-schooler abilities in 4 years. Tracing trendlines in compute (~0.5 orders of magnitude or OOMs/year), algorithmic efficiencies (~0.5 OOMs/year), and “unhobbling” gains (from chatbot to agent), we should expect another preschooler-to-high-schooler-sized qualitative jump by 2027. 

**II. [From AGI to Superintelligence: the Intelligence Explosion](https://situational-awareness.ai/from-agi-to-superintelligence/ "From AGI to Superintelligence: the Intelligence Explosion")**  
AI progress won’t stop at human-level. Hundreds of millions of AGIs could automate AI research, compressing a decade of algorithmic progress (5+ OOMs) into ≤1 year. We would rapidly go from human-level to *vastly* superhuman AI systems. The power—and the peril—of superintelligence would be dramatic.

**III. The Challenges**

**IIIa. [Racing to the Trillion-Dollar Cluster  
](https://situational-awareness.ai/racing-to-the-trillion-dollar-cluster/ "Racing to the Trillion-Dollar Cluster")**The most extraordinary techno-capital acceleration has been set in motion. As AI revenue grows rapidly, many trillions of dollars will go into GPU, datacenter, and power buildout before the end of the decade. The industrial mobilization, including growing US electricity production by 10s of percent, will be intense. 

**IIIb. [Lock Down the Labs: Security for AGI](https://situational-awareness.ai/lock-down-the-labs/ "Lock Down the Labs: Security for AGI")  
**The nation’s leading AI labs treat security as an afterthought. Currently, they’re basically handing the key secrets for AGI to the CCP on a silver platter. Securing the AGI secrets and weights against the state-actor threat will be an immense effort, and we’re not on track. 

**IIIc. [Superalignment](https://situational-awareness.ai/superalignment/ "Superalignment")**  
Reliably controlling AI systems much smarter than we are is an unsolved technical problem. And while it is a solvable problem, things could easily go off the rails during a rapid intelligence explosion. Managing this will be extremely tense; failure could easily be catastrophic.

**IIId.** [**The Free World Must Prevail**  
](https://situational-awareness.ai/the-free-world-must-prevail/ "The Free World Must Prevail")Superintelligence will give a decisive economic and military advantage. China isn’t at all out of the game yet. In the race to AGI, the free world’s very survival will be at stake. Can we maintain our preeminence over the authoritarian powers? And will we manage to avoid self-destruction along the way?

**IV. [The Project](https://situational-awareness.ai/the-project/)  
**As the race to AGI intensifies, the national security state will get involved. The USG will wake from its slumber, and by 27/28 we’ll get some form of government AGI project. No startup can handle superintelligence. Somewhere in a SCIF, the endgame will be on. 

**V. [Parting Thoughts](https://situational-awareness.ai/parting-thoughts/ "Parting Thoughts")**  
What if we’re right?

Next post in series:  
***[I.  From GPT-4 to AGI: Counting the OOMs](https://situational-awareness.ai/from-gpt-4-to-agi/)***

---

*While I used to work at OpenAI, all of this is based on publicly-available information, my own ideas,  general field-knowledge, or SF-gossip.* 

*Thank you to Collin Burns, Avital Balwit, Carl Shulman, Jan Leike, Ilya Sutskever, Holden Karnofsky, Sholto Douglas, James Bradbury, Dwarkesh Patel, and many others for formative discussions. Thank you to many friends for feedback on earlier drafts. Thank you to Joe Ronan for help with graphics, and Nick Whitaker for publishing help.*

*Dedicated to Ilya Sutskever.*